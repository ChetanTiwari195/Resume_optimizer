# LaTeX Compiler API (Resume_optimizer)

This repository exposes a small FastAPI app in `latex_api.py` that accepts LaTeX source and returns a compiled PDF (using `pdflatex`). The app now includes request logging and per-request raw compiler logs, a `return_logs` option to include logs in responses, and a 20-second compile timeout.

This project is intended to be used as a compile endpoint in an automation pipeline (for example, an n8n workflow) where an AI agent (ChatGPT) is used to generate LaTeX code from user-provided keywords and the API produces the final PDF.

## Contents

- `latex_api.py` — FastAPI application that accepts JSON { "latex": "..." } and returns a PDF file.
- `requirements.txt` — Python dependencies (FastAPI, uvicorn).
- `.gitignore` — ignores the `logs/` folder generated by the API.

## Prerequisites

- Python 3.8+ installed and on PATH.
- A LaTeX distribution with `pdflatex` installed and on PATH (MiKTeX or TeX Live) for Windows.

## Quick start (PowerShell)

1. Create and activate a virtual environment:

```powershell
.\.venv\Scripts\activate
# Activate in PowerShell
. .\.venv\Scripts\Activate.ps1
# If activation is blocked:
# Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
```

2. Install Python dependencies:

```powershell
pip install -r requirements.txt
```

3. Run the API with uvicorn (from repository root):

```powershell
python -m uvicorn latex_api:app --reload --host 127.0.0.1 --port 8000
```

The API will be available at `http://127.0.0.1:8000`.

## Example requests

PowerShell example (posts a small LaTeX document and writes the returned PDF to `output.pdf`):

```powershell
$latex = '{"latex":"\\documentclass{article}\\begin{document}Hello from LaTeX\\end{document}"}'
Invoke-RestMethod -Uri http://127.0.0.1:8000/compile -Method Post -ContentType 'application/json' -Body $latex -OutFile output.pdf
```

Notes:

- The request body may include an optional boolean `return_logs` (default: false). When `return_logs` is true, the API will include the last ~4KB of the pdflatex output in the JSON response (useful for debugging).
- On errors, the API returns a JSON `detail` object that includes a `request_id` and an `error` message; if `return_logs` is true the logs are included as well. Use `request_id` to correlate with server-side log files.

cURL example:

```bash
curl -X POST http://127.0.0.1:8000/compile \
	-H "Content-Type: application/json" \
	-d '{"latex":"\\\\documentclass{article}\\\\begin{document}Hello from LaTeX\\\\end{document}"}' --output output.pdf
```

Note: JSON escaping requires backslashes to be escaped in the string.

## Integrating with n8n + ChatGPT (typical flow)

Below is a common pattern when using n8n to build an automation that accepts keywords and returns a PDF resume by leveraging ChatGPT to generate LaTeX and this API to compile it.

1. Webhook (n8n) — receives user input (keywords, summary, template choice).
2. OpenAI / HTTP Request (n8n) — call ChatGPT with a prompt template that asks for LaTeX source for a resume using the provided keywords. Configure the node to return the LaTeX string in the response (plain text).
3. (Optional) Function/Set node — sanitize or adjust the LaTeX text (strip disallowed commands, set documentclass or packages if needed).
4. HTTP Request (n8n) — POST to this API `/compile` endpoint with JSON body { "latex": "<latex string>", "return_logs": false }.
   - Method: POST
   - URL: http://<server>:8000/compile
   - Headers: Content-Type: application/json
   - Body: Raw JSON with `latex` set to the LaTeX string produced by ChatGPT
   - Save response: enable storing binary data (so n8n saves `output.pdf` as a binary file in the workflow)
5. Return or store the `output.pdf` (upload to S3, attach to an email, return to the user via webhook response, etc.).

If you want the API to return compiler logs in the response (for debugging), set `return_logs` to `true` in the JSON body.

### Example ChatGPT prompt template (short)

Use this as the system/user prompt in n8n OpenAI node. Tweak styling and package choices as needed.

"""
Produce a complete LaTeX document for a single-page resume using the following keywords and details: {{keywords}}.

- Use `article` class and only common packages (geometry, hyperref, fontenc) so compilation with pdflatex succeeds.
- Keep LaTeX source minimal and self-contained. Do not include \write18 or shell-escape commands.
- Return only the LaTeX source (no explanation or markdown fences).
  """

Replace `{{keywords}}` with the input coming from the webhook.

## Security and production considerations

- Do not accept arbitrary LaTeX from untrusted users in production without sandboxing. LaTeX can execute commands or consume large resources.
- Add request size limits, compile timeouts, and run the compiler under an unprivileged user.
- Use subprocess timeouts and resource limits (e.g., ulimit or container cgroups) to avoid DoS by huge documents.
- Validate or sanitize the LaTeX input server-side: strip `\write18`, `\openout`, and other potentially dangerous primitives.
- Protect the compile endpoint with authentication (API key, JWT, or network-level protection), and use HTTPS in production.

Logging notes:

- The app creates a `logs/` directory (if missing) and writes an application log at `logs/latex_api.log`.
- For each compilation request the app writes a per-request raw compiler log to `logs/<request_id>.log` containing the `pdflatex` stdout/stderr. These files are useful for debugging errors.
- The repository includes a `.gitignore` entry for `logs/` so these runtime logs won't be committed.

Timeouts and errors:

- The server runs `pdflatex` with a 20-second timeout. If compilation times out the API returns HTTP 408.
- On compilation failures the API returns an object containing `status: "error"`, `request_id`, and `error` (and optionally `logs` when `return_logs` is true).

## Deployment and scaling notes

- For local testing, running the app with `uvicorn` is sufficient. For production, run behind a process manager (gunicorn + uvicorn workers) or containerize the app.
- Consider building a Docker image that includes a TeX distribution (MiKTeX or TeX Live). TeX distributions are large; a good approach is to use a base image that already includes TeX or to build a small image with only needed packages.
- Keep temporary directories isolated (the app currently uses Python's `tempfile.TemporaryDirectory`). Clean-up is automatic on success/exit, but robust error handling is still recommended.

## Troubleshooting

- If `pdflatex` is not found, install MiKTeX or TeX Live and ensure `pdflatex` is on PATH. Restart your shell/IDE after installation.
- If you get a 400 from the API, check uvicorn logs — the `detail` field contains the LaTeX compiler output and the `request_id` for correlating per-request log files.
- If activation of venv fails in PowerShell, run `Set-ExecutionPolicy -Scope Process -ExecutionPolicy RemoteSigned` then retry activation.

## Next steps (recommended)

- Add authentication (API key) to the compile endpoint and require it in the n8n HTTP Request node.
- Consider additional sandboxing for LaTeX and stricter input sanitization to prevent abuse.
- Add integration tests or an n8n workflow JSON demonstrating end-to-end use.

If you want, I can add simple API-key middleware, expand the logging format, or create an n8n workflow export you can import directly.

# LaTeX Compiler API (Resume_optimizer)

This repository exposes a small FastAPI app in `latex_api.py` that accepts LaTeX source and returns a compiled PDF (using `pdflatex`).

This project is intended to be used as a compile endpoint in an automation pipeline (for example, an n8n workflow) where an AI agent (ChatGPT) is used to generate LaTeX code from user-provided keywords and the API produces the final PDF.

## Contents

- `latex_api.py` — FastAPI application that accepts JSON { "latex": "..." } and returns a PDF file.
- `requirements.txt` — Python dependencies (FastAPI, uvicorn).

## Prerequisites

- Python 3.8+ installed and on PATH.
- A LaTeX distribution with `pdflatex` installed and on PATH (MiKTeX or TeX Live) for Windows.

## Quick start (PowerShell)

1. Create and activate a virtual environment:

```powershell
.\.venv\Scripts\activate
# Activate in PowerShell
. .\.venv\Scripts\Activate.ps1
# If activation is blocked:
# Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
```

2. Install Python dependencies:

```powershell
pip install -r requirements.txt
```

3. Run the API with uvicorn (from repository root):

```powershell
python -m uvicorn latex_api:app --reload --host 127.0.0.1 --port 8000
```

The API will be available at `http://127.0.0.1:8000`.

## Example requests

PowerShell example (posts a small LaTeX document and writes the returned PDF to `output.pdf`):

```powershell
$latex = '{"latex":"\\documentclass{article}\\begin{document}Hello from LaTeX\\end{document}"}'
Invoke-RestMethod -Uri http://127.0.0.1:8000/compile -Method Post -ContentType 'application/json' -Body $latex -OutFile output.pdf
```

cURL example:

```bash
curl -X POST http://127.0.0.1:8000/compile \
	-H "Content-Type: application/json" \
	-d '{"latex":"\\\\documentclass{article}\\\\begin{document}Hello from LaTeX\\\\end{document}"}' --output output.pdf
```

Note: JSON escaping requires backslashes to be escaped in the string.

## Integrating with n8n + ChatGPT (typical flow)

Below is a common pattern when using n8n to build an automation that accepts keywords and returns a PDF resume by leveraging ChatGPT to generate LaTeX and this API to compile it.

1. Webhook (n8n) — receives user input (keywords, summary, template choice).
2. OpenAI / HTTP Request (n8n) — call ChatGPT with a prompt template that asks for LaTeX source for a resume using the provided keywords. Configure the node to return the LaTeX string in the response (plain text).
3. (Optional) Function/Set node — sanitize or adjust the LaTeX text (strip disallowed commands, set documentclass or packages if needed).
4. HTTP Request (n8n) — POST to this API `/compile` endpoint with JSON body { "latex": "<latex string>" }.
   - Method: POST
   - URL: http://<server>:8000/compile
   - Headers: Content-Type: application/json
   - Body: Raw JSON with `latex` set to the LaTeX string produced by ChatGPT
   - Save response: enable storing binary data (so n8n saves `output.pdf` as a binary file in the workflow)
5. Return or store the `output.pdf` (upload to S3, attach to an email, return to the user via webhook response, etc.).

### Example ChatGPT prompt template (short)

Use this as the system/user prompt in n8n OpenAI node. Tweak styling and package choices as needed.

"""
Produce a complete LaTeX document for a single-page resume using the following keywords and details: {{keywords}}.

- Use `article` class and only common packages (geometry, hyperref, fontenc) so compilation with pdflatex succeeds.
- Keep LaTeX source minimal and self-contained. Do not include \write18 or shell-escape commands.
- Return only the LaTeX source (no explanation or markdown fences).
  """

Replace `{{keywords}}` with the input coming from the webhook.

## Payload examples

Example JSON body sent to this API:

```json
{
  "latex": "\\documentclass{article}\\begin{document}Hello\\end{document}"
}
```

If using n8n, make sure to pass the raw LaTeX string (no extra JSON encoding) into the HTTP Request body.

## Security and production considerations

- Do not accept arbitrary LaTeX from untrusted users in production without sandboxing. LaTeX can execute commands or consume large resources.
- Add request size limits, compile timeouts, and run the compiler under an unprivileged user.
- Use subprocess timeouts and resource limits (e.g., ulimit or container cgroups) to avoid DoS by huge documents.
- Validate or sanitize the LaTeX input server-side: strip `\write18`, `\openout`, and other potentially dangerous primitives.
- Protect the compile endpoint with authentication (API key, JWT, or network-level protection), and use HTTPS in production.

## Deployment and scaling notes

- For local testing, running the app with `uvicorn` is sufficient. For production, run behind a process manager (gunicorn + uvicorn workers) or containerize the app.
- Consider building a Docker image that includes a TeX distribution (MiKTeX or TeX Live). TeX distributions are large; a good approach is to use a base image that already includes TeX or to build a small image with only needed packages.
- Keep temporary directories isolated (the app currently uses Python's `tempfile.TemporaryDirectory`). Clean-up is automatic on success/exit, but robust error handling is still recommended.

## Troubleshooting

- If `pdflatex` is not found, install MiKTeX or TeX Live and ensure `pdflatex` is on PATH. Restart your shell/IDE after installation.
- If you get a 400 from the API, check uvicorn logs — the `detail` field contains the LaTeX compiler output.
- If activation of venv fails in PowerShell, run `Set-ExecutionPolicy -Scope Process -ExecutionPolicy RemoteSigned` then retry activation.

